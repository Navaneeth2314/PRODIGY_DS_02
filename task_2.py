# -*- coding: utf-8 -*-
"""Task 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kjFZHdUPTZ4p2GfSriTKo-TzqXWqHvcu
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option('display.max_columns',10,'display.width',1000) # corrected typo in option name
train  = pd.read_csv('/content/train.csv')
test = pd.read_csv('/content/test.csv')
train.head()

train.shape

test.shape

train.isnull().sum()

test.isnull().sum()

train.describe(include='all')

numeric_columns = train.select_dtypes(include=['number'])

train.groupby('Survived')[numeric_columns.columns].mean()

male_ind = len(train[train['Sex']=='male'])
print("No. of males in titanic",male_ind)

female_ind = len(train[train['Sex']=='female'])
print("No. of males in titanic",female_ind)

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
gender = ['Male','Female']
index = [577,314]
ax.bar(gender,index)

plt.title("Gender distribution")
plt.xlabel("Gender")
plt.ylabel("No. of people")
plt.show()

alive = len(train[train['Survived']==1])
dead  = len(train[train['Survived']==0])

train.groupby('Sex')[['Survived']].mean()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
status = ['Alive','Dead']
ind   = [alive,dead]
ax.bar(status,ind)
plt.xlabel("Status")
plt.ylabel("No. of people")
plt.title("Status distribution")
plt.show()

plt.figure(1)
train.loc[train['Survived'] == 1,'Pclass'].value_counts().plot(kind='bar')
plt.title('Bar graph of people  accoeding to ticket  class in which people survived')

plt.figure(2)
train.loc[train['Survived'] == 0 ,'Pclass'].value_counts().plot(kind='bar')
plt.title('Bar graph of people  accoeding to ticket  class in which people didn\'t  survived')

plt.figure(1)
age = train.loc[train.Survived == 1,'Age']
plt.title('Histogram of people  accoeding to age in which people survived')
plt.hist(age,np.arange(0,100,10))
plt.xticks(np.arange(0,100,10))

plt.figure(2)
age = train.loc[train.Survived == 0,'Age']
plt.title('Histogram of people  accoeding to age in which didn\'t people survived')
plt.hist(age,np.arange(0,100,10))
plt.xticks(np.arange(0,100,10))

train[["SibSp", "Survived"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived',ascending = False)

train[["Pclass", "Survived"]].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived',ascending = False)

train[["Age", "Survived"]].groupby(['Age'], as_index=False).mean().sort_values(by='Survived',ascending = False)

train[["Embarked", "Survived"]].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived',ascending = False)

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.axis('equal')
l = ['C = Cherbourg','Q = Queenstown','S = Southampton']
s = [0.55,0.38,0.33]
ax.pie(s, labels = l,autopct='%1.2f%%')
plt.show()

test.describe(include='all')

train = train.drop(['Ticket'],axis = 1)
test = test.drop(['Ticket'],axis = 1)

train = train.drop(['Cabin'],axis = 1)
test = test.drop(['Cabin'],axis = 1)

train = train.drop(['Name'],axis = 1)
test = test.drop(['Name'],axis = 1)

column_train=['Age','Embarked','Fare','Parch','Pclass','SibSp','Sex']

X= train[column_train]
Y= train['Survived']

X['Age'].isnull().sum()
X['Embarked'].isnull().sum()
X['Fare'].isnull().sum()
X['Parch'].isnull().sum()
X['Pclass'].isnull().sum()
X['SibSp'].isnull().sum()
X['Sex'].isnull().sum()

X['Age'].fillna(X['Age'].median())
X['Age'].isnull().sum()

X['Embarked'].fillna(X['Embarked'].mode()[0], inplace=True) # Use mode for categorical data
X['Embarked'].isnull().sum()

# Check unique values in 'Sex' column before applying the mapping
print(X['Sex'].unique())

d ={'male':0,'female':1}
X['Sex']=X['Sex'].apply(lambda x:d.get(x,x)) # Use get method to handle potential non-existent keys
X['Sex'].head()

X['Embarked'] = X['Embarked'].fillna(X['Embarked'].mode()[0]) # Fill missing values with the most frequent value
e = {'C':0,'Q':1,'S':2}
X['Embarked'] = X['Embarked'].map(e) # Convert categorical values to numerical using the mapping

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=7)

# Impute missing values for all numerical features
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')  # Use mean imputation for numerical columns
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

# Now fit the model
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
model = LogisticRegression()
model.fit(X_train, Y_train)
Y_pred = model.predict(X_test)

print("Accuracy score:",accuracy_score(Y_test, Y_pred))

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(Y_test, Y_pred)
print(cm)

from sklearn.svm import SVC
model = SVC()
model.fit(X_train, Y_train)
Y_pred = model.predict(X_test)

print("Accuracy score:",accuracy_score(Y_test, Y_pred))

from sklearn.metrics import classification_report
print(classification_report(Y_test, Y_pred))

from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
model.fit(X_train, Y_train)
Y_pred = model.predict(X_test)

print("Accuracy score:",accuracy_score(Y_test, Y_pred))

from sklearn.metrics import classification_report
print(classification_report(Y_test, Y_pred))

from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier()
model.fit(X_train, Y_train)
Y_pred = model.predict(X_test)

print("Accuracy score:",accuracy_score(Y_test, Y_pred))

results = pd.DataFrame({
    'Model': ['Logistic Regression','Support Vector Machines', 'Naive Bayes','Decision Tree'],
    'Score': [.75,.66,.75,.75]})

result_df = results.sort_values(by='Score', ascending=False)
result_df = result_df.set_index('Score')
result_df.head(9)

results = pd.DataFrame({
    'Model': ['Logistic Regression','Support Vector Machines', 'Naive Bayes','Decision Tree'],
    'Score': [.75,.66,.75,.75]})

result_df = results.sort_values(by='Score', ascending=False)

# Plotting the results in a bar graph
plt.figure(figsize=(10, 6))
sns.barplot(x='Score', y='Model', data=result_df, palette='viridis')
plt.title('Model Comparison by Accuracy Score')
plt.xlabel('Accuracy Score')
plt.ylabel('Model')
plt.xlim(0, 1)  # Assuming accuracy scores are between 0 and 1
plt.show()